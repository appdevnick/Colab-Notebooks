{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appdevnick/Colab-Notebooks/blob/main/rag_hands_on_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hands-On with RAG: A Practical Guide Using Legacy Protocols\n",
        "\n",
        "This notebook contains the exact same code presented in the [blog post](https://nickslinuxlearnings.com/posts/Hands-On-with-RAG-A-Practical-Guide/). You can run each cell to see RAG in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with compatible versions\n",
        "!pip install -q langchain-core==0.1.21 langsmith==0.0.83 langchain-community==0.0.20 langchain-openai openai chromadb python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# For Colab, we'll use getpass instead of .env file\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository to get the documentation files\n",
        "!git clone https://github.com/appdevnick/Colab-Notebooks.git\n",
        "!mv Colab-Notebooks/telecom_docs ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import glob\n",
        "\n",
        "# Load all detailed text files individually\n",
        "docs = []\n",
        "for file_path in glob.glob('telecom_docs/*detailed.txt'):\n",
        "    loader = TextLoader(file_path)\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "# Split into chunks - adjusted for longer, more detailed content\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,  # Increased for more context\n",
        "    chunk_overlap=400,  # Increased to maintain context\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # More granular separation\n",
        ")\n",
        "texts = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Create embeddings and store in vector database\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Set up the RAG pipeline with a more detailed prompt\n",
        "llm = ChatOpenAI(temperature=0.2)\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 3}  # Retrieve more context\n",
        ")\n",
        "\n",
        "# Enhanced prompt template for more detailed responses\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Answer the question based on the following context. If the information is available in the context, provide specific details, examples, and technical specifications. If certain aspects of the answer cannot be derived from the context, clearly indicate what information is missing.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        ")\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare RAG vs Base Model responses\n",
        "def compare_responses(question):\n",
        "    print(f\"Question: {question}\\n\")\n",
        "    \n",
        "    # Base model response\n",
        "    print(\"Base Model Response:\")\n",
        "    base_response = llm.invoke(question).content\n",
        "    print(f\"{base_response}\\n\")\n",
        "    \n",
        "    # RAG-enhanced response\n",
        "    print(\"RAG Response:\")\n",
        "    rag_response = rag_chain.invoke(question)\n",
        "    print(f\"{rag_response}\\n\")\n",
        "    \n",
        "    print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# Test questions focusing on technical details and comparisons\n",
        "questions = [\n",
        "    \"What's the T1 timer in X.25 and why does it vary between vendors?\",\n",
        "    \"Compare NTT vs AT&T Accunet timer values and explain the reasons for their differences\",\n",
        "    \"Explain the different frame types in X.25 and their specific roles in data transmission\",\n",
        "    \"How does X.25 implement error recovery for satellite vs terrestrial links?\",\n",
        "    \"What are the key differences in how AT&T Accunet and NTT handle international connections?\",\n",
        "    \"Describe the performance impact of T1 timer settings in X.25 networks\"\n",
        "]\n",
        "\n",
        "print(\"Testing RAG system with detailed documentation...\\n\")\n",
        "for question in questions:\n",
        "    compare_responses(question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
